{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFNN-lab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y1vJpkWkfhof",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGxXhuuY_cQ5",
        "colab_type": "text"
      },
      "source": [
        "**activation functions**\n",
        "\n",
        "Let's start by writing some functions for activation functions that we would like to be able to use.\n",
        "\n",
        "Fill in the functions below to implement the associated activation functions. Any time you need a special function (e.g. exponentation), try to find a version in NumPy so that your activation functions will work on single values as well as arrays.\n",
        "\n",
        "*bonus*: try to implment the ReLU activation function so that it works elementwise on a NumPy -- this is called \"vectorizing\" your code. Hint: check out the `np.where` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YYCjq4olfhoy",
        "colab": {}
      },
      "source": [
        "def linear(z):\n",
        "  '''\n",
        "  linear activation function\n",
        "  '''\n",
        "  pass\n",
        "\n",
        "# more specifically, the logistic sigmoid that has values between 0 and 1\n",
        "def sigmoid(z):\n",
        "  '''\n",
        "  sigmoid activation function\n",
        "  '''\n",
        "  pass\n",
        "\n",
        "def tanh(z):\n",
        "  '''\n",
        "  tanh activation function\n",
        "  '''\n",
        "  pass\n",
        "\n",
        "# update this to work on a NumPy array without using a loop\n",
        "def relu(z):\n",
        "  if z < 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return z\n",
        "\n",
        "def relu(z):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws4x-FkNgKso",
        "colab_type": "text"
      },
      "source": [
        "tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocZ7dSZ0GzRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.array([-1.0, 0.0, 1.0])\n",
        "\n",
        "np.testing.assert_equal(linear(5), 5)\n",
        "np.testing.assert_equal(linear(-3.0), -3.0)\n",
        "np.testing.assert_array_equal(linear(a), a)\n",
        "\n",
        "np.testing.assert_equal(sigmoid(0.0), 0.5)\n",
        "np.testing.assert_allclose(sigmoid(a), [0.26894142, 0.5, 0.73105858])\n",
        "\n",
        "np.testing.assert_equal(tanh(0.0), 0.0)\n",
        "np.testing.assert_allclose(tanh(a), [-0.76159416, 0.0, 0.76159416])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KTTLMS_mfhpD"
      },
      "source": [
        "**single neuron**\n",
        "\n",
        "As a warm-up for implementing a full neural network layer, let's implement the computation performed by a single neuron.\n",
        "\n",
        "If our neuron has $m$ inputs, then its output will be defined by:\n",
        "\n",
        "- the inputs, which we can represent as an $m$-dimensional array: $x=[x_1, x_2, ..., x_m]$\n",
        "- the weights for each input, which we can represent as another $m$-dimensional vector: $w = [w_1, w_2, ..., w_n]$\n",
        "- the bias, which is a scalar: $b$\n",
        "- the activation function: $f$\n",
        "\n",
        "With these definitions, the output of our neuron is:\n",
        "$$a = f(w_1 x_1 + w_2 x_2 + ... + w_m x_m + b)$$\n",
        "$$=f(w \\cdot x + b)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyOApBlhBMFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neuron(x, w, b, f):\n",
        "  '''\n",
        "  tranformation for a single layer of a neural network\n",
        "\n",
        "  parameters\n",
        "  ----------\n",
        "  x (1d array): input vector\n",
        "  w (1d array): neuron weights\n",
        "  b (float):    bias\n",
        "  f (func):     activation function\n",
        "  '''\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz8s4uCTgQJv",
        "colab_type": "text"
      },
      "source": [
        "tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9XwKGtRLiqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.testing.assert_equal(\n",
        "    neuron([0, 0, 0], [0, 0, 0], 10, linear),\n",
        "    10\n",
        ")\n",
        "np.testing.assert_equal(\n",
        "    neuron([1, 2, 3], [0, 1, 0], 0, linear),\n",
        "    2\n",
        ")\n",
        "np.testing.assert_equal(\n",
        "    neuron([1, 2, 3], [1, 2, 3], 0, linear),\n",
        "    14\n",
        ")\n",
        "np.testing.assert_equal(\n",
        "    neuron([1, 1, 1], [-1, 0, 1], 0, linear),\n",
        "    0\n",
        ")\n",
        "np.testing.assert_equal(\n",
        "    neuron([-1, 0, 1], [1, 1, 1], 0, linear),\n",
        "    0\n",
        ")\n",
        "np.testing.assert_equal(\n",
        "    neuron([0, 0, 0], [1, 1, 1], -1, tanh),\n",
        "    np.tanh(-1.0)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0IV8Z3DNYRc",
        "colab_type": "text"
      },
      "source": [
        "**vectorizing**\n",
        "\n",
        "If your code for a single neuron used a loop, go back and try to write it without, using helper functions from NumPy that let you avoid writing a loop. Hint: check out the NumPy's `np.dot` function.\n",
        "\n",
        "This is known as \"vectorizing\" your code and, in general, is a good idea in a language like Python which is not built for speed. Fortunately, libraries like NumPy have critical portions of their code written in lower level languages (like C, C++, or FORTRAN) where loops are faster (though the code is harder to write and make sure it is correct)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvZLOi_wUju2",
        "colab_type": "text"
      },
      "source": [
        "**neural net layer with batch inputs**\n",
        "\n",
        "Now for the real deal: extending our function for a single neuron in two ways:\n",
        "- first, so that it can process a \"batch\" of multiple inputs rather than just a single input\n",
        "- second, so that it can do the computation for an entire hidden layer of neurons instead of just one\n",
        "\n",
        "Let's look at the math for each of these extensions in turn.\n",
        "\n",
        "*batch inputs*:\n",
        "\n",
        "We want to process a set (or \"batch\") of inputs at the same time, both for the sake of convenience and so we can hopefully vectorize the code to make processing a batch faster than simply looping through all of the inputs in the batch.\n",
        "\n",
        "If we have $k$ inputs in a batch, each of which are $m$-dimensional, then we can organize these inputs into a $k$-by-$m$ matrix simply by stacking the individual input vectors:\n",
        "\n",
        "\\\\\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "  -\\, x^1 \\, -\\\\\n",
        "  -\\, x^2 \\, -\\\\\n",
        "  \\vdots \\\\\n",
        "  -\\, x^k \\, -\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} & \\cdots & x_{1m} \\\\\n",
        "x_{21} & x_{22} & \\cdots & x_{2m} \\\\\n",
        "\\vdots & \\vdots  &  \\ddots & \\vdots \\\\\n",
        "x_{k1} & x_{k2} & \\cdots & x_{km}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\\\\\n",
        "where each row contains the components for one input\n",
        "\n",
        "*multiple neurons*\n",
        "\n",
        "Now let's considering an entire layer of neurons that recieve this same input and, again, we want to compute the outputs for the entire layer together for convenience and efficiency.\n",
        "\n",
        "If we have $n$ neurons, we will now have $m$ of weights (for a  $m$-dimensional input) for each neuron. Similar to what we just saw for extending to multiple inputs, we can collect these weights in an $m$-by-$n$ matrix, but this times lets put each weight vector in its own column (we'll see why shortly):\n",
        "\n",
        "\\\\\n",
        "$$W =\n",
        "\\begin{bmatrix}\n",
        "| & | & \\quad & | \\\\\n",
        "w^1 & w^2 & \\cdots & w^n \\\\\n",
        "| & | & \\quad & |\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "w_{11} & w_{12} & \\cdots & w_{1n} \\\\\n",
        "w_{21} & w_{22} & \\cdots & w_{2n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "w_{m1} & w_{m2} & \\cdots & w_{mn}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\\\\\n",
        "where each column contains the weights for one of the neurons in the layer. We also have one bias for each neuron, which we can organize in a vector:\n",
        "\n",
        "\\\\\n",
        "$$\n",
        "b =\n",
        "\\begin{bmatrix}\n",
        "b_1 & b_2 & \\cdots & b_n\n",
        "\\end{bmatrix}\n",
        "$$.\n",
        "\n",
        "\\\\\n",
        "*putting it all together*\n",
        "\n",
        "Next, we want to compute the total weighted input to each neuron in the layer for each input. We can organize these values into a matrix, with one row for each sample and one column for each neuron in our layer. \n",
        "\n",
        "\\\\\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x^1 \\cdot w^1 & x^1 \\cdot w^2 & \\cdots & x^1 \\cdot w^n \\\\\n",
        "x^2 \\cdot w^1 & x^2 \\cdot w^2 & \\cdots & x^2 \\cdot x^n \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x^m \\cdot w^1 & x^m \\cdot w^2 & \\cdots & x^m \\cdot w^n\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\\\\\n",
        "But taking the dot product between all rows of one matrix with all columns of another matrix is exactly the definition of matrix multiplication, so we have:\n",
        "\n",
        "\\\\\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x^1 \\cdot w^1 & x^1 \\cdot w^2 & \\cdots & x^1 \\cdot w^n \\\\\n",
        "x^2 \\cdot w^1 & x^2 \\cdot w^2 & \\cdots & x^2 \\cdot x^n \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x^m \\cdot w^1 & x^m \\cdot w^2 & \\cdots & x^m \\cdot w^n\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "  -\\, x^1 \\, -\\\\\n",
        "  -\\, x^2 \\, -\\\\\n",
        "  \\vdots \\\\\n",
        "  -\\, x^k \\, -\\\\\n",
        "\\end{bmatrix} \n",
        "\\begin{bmatrix}\n",
        "| & | & \\quad & | \\\\\n",
        "w^1 & w^2 & \\cdots & w^n \\\\\n",
        "| & | & \\quad & |\n",
        "\\end{bmatrix}\n",
        "=XW\n",
        "$$\n",
        "\n",
        "\\\\\n",
        "We also need to add the bias terms, which is the same for each row/neuron. We can write this as:\n",
        "\n",
        "\\\\\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x^1 \\cdot w^1 + b_1 & x^1 \\cdot w^2 + b_2 & \\cdots & x^1 \\cdot w^n + b_n \\\\\n",
        "x^2 \\cdot w^1 + b_1 & x^2 \\cdot w^2 + b_2 & \\cdots & x^2 \\cdot x^n + b_n \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x^m \\cdot w^1 + b_1 & x^2 \\cdot w^2 + b_2 & \\cdots & x^m \\cdot w^n + b_n\n",
        "\\end{bmatrix} \\\\\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "x^1 \\cdot w^1 & x^1 \\cdot w^2 & \\cdots & x^1 \\cdot w^n \\\\\n",
        "x^2 \\cdot w^1 & x^2 \\cdot w^2 & \\cdots & x^2 \\cdot x^n \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x^m \\cdot w^1 & x^m \\cdot w^2 & \\cdots & x^m \\cdot w^n\n",
        "\\end{bmatrix}\n",
        "+\n",
        "\\begin{bmatrix}\n",
        "b_1 & b_2 & \\cdots & b_m\n",
        "\\end{bmatrix}\n",
        "= XW + b\n",
        "$$\n",
        "\n",
        "\\\\\n",
        "where when have have addition of a matrix to a row/column vector, we interpret this via *broadcasting*: we assume that the row/column is repeated across all rows/columns to match the shape of the matrix. This is exactly how many numeric programming langauges/packages (including NumPy) handle this as well.\n",
        "\n",
        "Finally, the outputs for all neurons across all batches can be computed by applying the activation function elementwise for a final result of:\n",
        "\n",
        "\\\\\n",
        "$$\n",
        "A = f(XW + b)\n",
        "$$\n",
        "\n",
        "\\\\\n",
        "And here is the best part: the same function in NumPy that computes the dot-product between two 1-dimensional arrays also computes matrix multiplication when given 2-dimensional arrays instead. This means that the function you wrote for a single neuron will already work for multiple neurons across a batch of inputs!\n",
        "\n",
        "Go ahead and try it -- simply change the weights and inputs to 2d-arrays (and you can change the biases to a 1d-array if you want a different bias for each neuron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLsba8kjstTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nn_layer(X, W, b, f):\n",
        "  '''\n",
        "  tranformation for a single layer of a neural network\n",
        "\n",
        "  parameters\n",
        "  ----------\n",
        "  X (2d array): input vectors\n",
        "  W (2d array): neuron weights\n",
        "  b (1d array): biases\n",
        "  f (func):     activation function\n",
        "  '''\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qzyq8a4gdu7",
        "colab_type": "text"
      },
      "source": [
        "tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liQhbBRQsEQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = [\n",
        "     [0, 0, 0],\n",
        "     [1, 1, 1],\n",
        "     [-1, 0, 1]\n",
        "]\n",
        "W = [\n",
        "     [0, 1, 0],\n",
        "     [0, 1, 1],\n",
        "     [0, 1, 2]\n",
        "]\n",
        "b = [0, 0, 0]\n",
        "A = nn_layer(X, W, b, linear)\n",
        "np.testing.assert_array_equal(A, [[0, 0, 0], [0, 3, 3], [0, 0, 2]])\n",
        "\n",
        "b = [[1, 2, 3]]\n",
        "A = nn_layer(X, W, b, linear)\n",
        "np.testing.assert_array_equal(A, [[1, 2, 3], [1, 5, 6], [1, 2, 5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv-XBlDHgTE8",
        "colab_type": "text"
      },
      "source": [
        "**neural net with one hidden layer**\n",
        "\n",
        "Next, let's make a function to implement a two-layer neural network - one hidden layer and the output layer. It should takes as inputs:\n",
        "- An input matrix, $X$\n",
        "- Two weight matrices, $W_1$ and $W_2$ (one for each layer)\n",
        "- Two bias vectors, $b_1$ and $b_2$ (one for each layer)\n",
        "- Two activation functions (one for the hidden layer, one for the output)\n",
        "\n",
        "This function should call your `nn_layer` function twice with the appropriate arguments, chaining the output of the first layer into the input to the secocond layer, and then returning the final output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGZ2Kw-JhcL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nn_two_layers(X, W_1, b_1, f_1, W_2, b_2, f_2):\n",
        "  '''\n",
        "  forward pass of a two-layer neural network\n",
        "\n",
        "  parameters\n",
        "  ----------\n",
        "  X (2d array):   input matrix\n",
        "  W_1 (2d array): weight matrix for first layer\n",
        "  b_1 (1d array): biases for first layer\n",
        "  f_1 (func):     activation function for first layer\n",
        "  W_2 (2d array): weight matrix for second layer\n",
        "  b_2 (1darray):  biases for second layer\n",
        "  f_2 (func):     activtion function for second/output layer\n",
        "  '''\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ledcpsf7xpBx",
        "colab_type": "text"
      },
      "source": [
        "To test out your brand new neral net function, let's test it out on a 1-dimensional input and a 1-dimensional output so that we can plot the results with a line graph. We will use random values for the weight and biases.\n",
        "\n",
        "Follow along below as we walk through how to do this with some NumPy functions and Python's popular plotting package, Matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfWMCAfqiCU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the input dimension, the number of hidden units, and the number of ouptput units\n",
        "n_input, n_hidden, n_output = 1, 5, 1\n",
        "\n",
        "# We want get the NN's output for a range of input values, so that we cant plot\n",
        "# input vs output. We can get evenly space values using `np.linspace`. We also\n",
        "# want to process these inputs as a \"batch\", so we use `np.newaxis` to turn this\n",
        "# 1-d array into a 2-d array with a single column.\n",
        "n_grid = 100\n",
        "X = np.linspace(-10, 10, n_grid)[:, np.newaxis]\n",
        "\n",
        "# We can generate random values (drawn from a standard gaussian distribution --\n",
        "# mean = 0, standard deviation = 1), with `np.random.randn(shape)`\n",
        "W_1 = np.random.randn(n_input, n_hidden)\n",
        "b_1 = np.random.randn(n_hidden)\n",
        "W_2 = np.random.randn(n_hidden, n_output)\n",
        "b_2 = np.random.randn(n_output)\n",
        "\n",
        "# Use our NN to compute the outputs for these inputs\n",
        "Y_hat = nn_two_layers(X, W_1, b_1, tanh, W_2, b_2, linear)\n",
        "\n",
        "# Plot the input values (x-axis) against the output values (y-axis)\n",
        "plt.plot(X, Y_hat)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Rz0H0hefhqY"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UPtF4qwgfhqa"
      },
      "source": [
        "**function exploration**\n",
        "\n",
        "Experiment with your 2-layer neural network using the code above to answer the following questions.\n",
        "\n",
        "1. Use a `relu` activation on the hidden layer and a `linear` activation on the output layer. Now vary the number of hidden unites between, say 1 and 20. How does the number of hidden units effect the function that your neural net represents? Why is this?\n",
        "\n",
        "2. Now keep using a `linear` output activation, but compare and contrast using `relu` vs `tanh` for the activation on the hidden layer. What do you notice is the main difference between the represented functions? Why is this?\n",
        "\n",
        "3. Finally, switch to using a `sigmoid` for the output activation. What values does restrict the output to? What type of supervised learning problem would this be ideally suited for?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K_NRZMnXfhq3"
      },
      "source": [
        "**deep neural nets**\n",
        "\n",
        "Our neural network currently works for only a single hidden layer. This is a rather shallow network, and the next step is to extend it to a deep network that can handle any number of layers.\n",
        "\n",
        "The handle an arbitrary number of layers, switch to taking a 3-dimensional array for the layer weights -- now the first index will specify which layer the remaing 2-dimension subarray is associated with (e.g. `W[2]` would be the 2-d array of weights for the connections from layer 2 to layer 3). Similary, add another dimension to the biases to specify the layer, making $b$ a 2-dimensional array.\n",
        "\n",
        "Here you will probably need to use a for loop to iterate through the layers. You will probably want to handle the final/output layer separately, as it needs its own activation function (so your code can match different output targets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShtDEGnUh7Xm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deep_nn(X, W, b, f_hidden, f_output):\n",
        "  '''\n",
        "  forward pass for a deep neural net with an arbitrary number of layers\n",
        "\n",
        "  parameters\n",
        "  ----------\n",
        "  X (2d array):     input vectors\n",
        "  W (3d array):     weight matrices -- W[layer, input, neuron]\n",
        "  b (2d array):     biase vectors -- b[layer, neuron]\n",
        "  f_hidden (func):  activation function for all hidden layers\n",
        "  f_output (func):  activation function for final/output layer\n",
        "  '''\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}